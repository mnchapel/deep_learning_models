---
title: R-CNN
author: Marie-Neige Chapel
date: 2014-06-24
category: CNN
layout: post
---

R-CNN, for Region with CNN features, is a deep learning architecture used for object detection. This architecture is composed of three modules: region proposal, feature extraction and object classification.

## Architecture

{% include model_architecture.html model='r-cnn' img_detailed='assets/img/rcnn_detailed.svg' %}

## Region Proposal

The region proposal module aim to localize and segment objects into bounding boxes. As explained by the authors [1], *"While R-CNN is agnostic to the particular region proposal method, we use selective search"*. This method will not be developed here and let the reader referred to the article [2]. For each image, around 2000 bounding boxes are extracted.

## Feature extraction

For each bounding box extracted by the previous module, a 4096-d feature vector is computed by a CNN. The authors have chosen to use AlexNet to achieve this task. Since the boudning boxes have non-fixed size, they are warp into 227x227 size because the CNN requires input data of a fixed size.

The neural network is trained in two stages. First, the dataset ILSVRC 2012 [3] is used for pre-training. This dataset contains only image-level annotations, no bounding box labels. Then the network is fine tune with on warped region proposals on images from the VOC dataset. In order to adapt the CNN to this new dataset, the last layer which ouput 1000 classes (for ImageNet) is replaced by a 21 classes output layer (20 classes from VOC dataset + 1 for the background class). Once the training is finished, the last layer is removed and the SVMs are used to select the corresponding class for each bounding box.

## Object Classification

Each bounding box is classified with one of the 21 classes (20 classes from VOC dataset + 1 for the background class) by using SVMs. There is one SVM per class. Finally, the bounding boxes are cleaned with a greedy non-maximum suppression applied on each class independently. A bounding box is removed it has intersection-over-union (IoU) overlap with another bounding box and the latter has a higher scoring selected region larger than a learned threshold. The image below presents an example of IoU overlap between two bounding boxes.

![IoU]({{site.baseurl}}/assets/img/iou.svg)

The hatch part on the image above represents the IoU while the blue and the red rectangles represent the bounding boxes. If we imagine that the red box has a higher score than the blue one, and even above than the learned threshold, then the blue box will be deleted.

## Bibliography

- [1] [[Paper] Rich feature hierarchies for accurate object detection and semantic segmentation](https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)
- [2] [[Paper] Selective Search for Object Recognition](http://vision.stanford.edu/teaching/cs231b_spring1415/papers/selsearch.pdf)
- [3] [[Challenge] ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012)](https://www.image-net.org/challenges/LSVRC/2012/)
