## Fully Connected

As explained by the authors in their paper [1] all the fully connected layers are followed by a rectified linear activation (ReLU).

![Convolution and ReLu]({{site.baseurl}}/assets/img/fc_relu.svg)
