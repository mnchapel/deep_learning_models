## Convolution

As explained by the authors in their paper [1] all the convolution are followed by a batch normalization and a rectified linear activation (ReLU).

![Convolution and ReLu]({{site.baseurl}}/assets/img/conv_bn_relu.svg)
